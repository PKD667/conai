{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pkd667/conai\n",
        "%cd conai"
      ],
      "metadata": {
        "id": "wvJIQ90o7XrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k-boEJHr6vk1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vbGEPLtT6vk2",
        "outputId": "017d7ee0-48f5-49dc-b68f-32290be4c58d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'typed_lambda_parser'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-dd68ac77fa8e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtyped_lambda_parser\u001b[0m \u001b[0;31m# Assuming this file exists and is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'typed_lambda_parser'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import typed_lambda_parser # Assuming this file exists and is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoZX5eWC6vk2"
      },
      "outputs": [],
      "source": [
        "# Import wrappers from the new file\n",
        "from hf_wrappers import TransformersTokenizerWrapper, TransformersModelWrapper\n",
        "\n",
        "from sampler import simplified_sampler,lambda_grammar_parse_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR25OcjS6vk5"
      },
      "outputs": [],
      "source": [
        "# --- Use Hugging Face Transformers model and tokenizer ---\n",
        "# model_name = \"gpt2\" # Example: GPT-2\n",
        "#model_name = \"EleutherAI/gpt-neo-125M\" # Example: GPT-Neo\n",
        "#model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Example: TinyLlama, ensure you have transformers>=4.34 for Llama2 tokenizers\n",
        "#model_name = \"google/gemma-3-4b-it\"\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "print(f\"Loading Hugging Face tokenizer: {model_name}\")\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(model_name,token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
        "print(f\"Loading Hugging Face model: {model_name}\")\n",
        "# Specify torch_dtype for model loading if desired, e.g., torch.bfloat16 or torch.float16\n",
        "# Ensure the chosen dtype is supported by the model and hardware.\n",
        "model_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32\n",
        "print(f\"Using model dtype: {model_dtype}\")\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
        "    torch_dtype=model_dtype # Use the determined dtype\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F6StsDmM6vk5",
        "outputId": "6922cf13-ada2-4cb5-e714-aebbd73cfe5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hf_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a3e8784e3052>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set device (CUDA if available, otherwise CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using device: {device_str}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hf_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Set device (CUDA if available, otherwise CPU)\n",
        "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "hf_model.to(device_str)\n",
        "\n",
        "print(f\"Using device: {device_str}\")\n",
        "# Wrap the Hugging Face components for compatibility with simplified_sampler\n",
        "wrapped_tokenizer = TransformersTokenizerWrapper(hf_tokenizer)\n",
        "# Pass the device string; TransformersModelWrapper will create torch.device(device_str)\n",
        "wrapped_model = TransformersModelWrapper(hf_model, device_str)\n",
        "wrapped_modeL.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58xDpbzy6vk5"
      },
      "outputs": [],
      "source": [
        "# do some testing\n",
        "# Example input\n",
        "input_text = \"Write 2 in church encoding.\"\n",
        "\n",
        "\n",
        "content = wrapped_model.infer(prompt_str=input_text,tokenizer=wrapped_tokenizer)\n",
        "print(content)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}