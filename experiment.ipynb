{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typed_lambda_parser # Assuming this file exists and is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import wrappers from the new file\n",
    "from hf_wrappers import TransformersTokenizerWrapper, TransformersModelWrapper\n",
    "\n",
    "from sampler import simplified_sampler,lambda_grammar_parse_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Use Hugging Face Transformers model and tokenizer ---\n",
    "# model_name = \"gpt2\" # Example: GPT-2\n",
    "#model_name = \"EleutherAI/gpt-neo-125M\" # Example: GPT-Neo\n",
    "#model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Example: TinyLlama, ensure you have transformers>=4.34 for Llama2 tokenizers\n",
    "#model_name = \"google/gemma-3-4b-it\"\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "print(f\"Loading Hugging Face tokenizer: {model_name}\")\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_name,token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "print(f\"Loading Hugging Face model: {model_name}\")\n",
    "# Specify torch_dtype for model loading if desired, e.g., torch.bfloat16 or torch.float16\n",
    "# Ensure the chosen dtype is supported by the model and hardware.\n",
    "model_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32\n",
    "print(f\"Using model dtype: {model_dtype}\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "    torch_dtype=model_dtype # Use the determined dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (CUDA if available, otherwise CPU)\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device_str}\")\n",
    "# Wrap the Hugging Face components for compatibility with simplified_sampler\n",
    "wrapped_tokenizer = TransformersTokenizerWrapper(hf_tokenizer)\n",
    "# Pass the device string; TransformersModelWrapper will create torch.device(device_str)\n",
    "wrapped_model = TransformersModelWrapper(hf_model, device_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some testing\n",
    "# Example input\n",
    "input_text = \"Write 2 in church encoding.\"\n",
    "# inference\n",
    "# Assuming the model is a causal language model\n",
    "# Use the model to generate predictions\n",
    "# Note: The model should be in evaluation mode (set in TransformersModelWrapper)\n",
    "# wrapped_model.eval() # Already called in TransformersModelWrapper.__init__\n",
    "\n",
    "content = wrapped_model.infer(prompt_str=input_text,tokenizer=wrapped_tokenizer) \n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
